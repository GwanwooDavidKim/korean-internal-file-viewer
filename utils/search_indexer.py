# -*- coding: utf-8 -*-
"""
ê²€ìƒ‰ ì¸ë±ì„œ ëª¨ë“ˆ (Search Indexer)

íŒŒì¼ ë‚´ìš©ì„ ì¸ë±ì‹±í•˜ê³  ë¹ ë¥¸ ì „ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import re
import json
import time
import threading
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import config
from utils.file_manager import FileManager


class SearchIndex:
    """
    ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    íŒŒì¼ì˜ ë‚´ìš©ì„ í† í°í™”í•˜ì—¬ ì—­ ì¸ë±ìŠ¤(inverted index)ë¥¼ êµ¬ì¶•í•˜ê³ ,
    ë¹ ë¥¸ ì „ë¬¸ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """SearchIndex ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.index = defaultdict(set)  # ë‹¨ì–´ -> íŒŒì¼ ê²½ë¡œ ì§‘í•©
        self.file_info = {}  # íŒŒì¼ ê²½ë¡œ -> íŒŒì¼ ì •ë³´
        self.stop_words = self._load_stop_words()
        self.lock = threading.RLock()
    
    def _load_stop_words(self) -> Set[str]:
        """ë¶ˆìš©ì–´ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        # í•œêµ­ì–´ì™€ ì˜ì–´ ê¸°ë³¸ ë¶ˆìš©ì–´
        korean_stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
            'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”',
            'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ'
        }
        
        english_stop_words = {
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'will', 'with', 'or', 'but', 'if', 'this', 'they'
        }
        
        return korean_stop_words | english_stop_words
    
    def _tokenize(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: í† í° ëª©ë¡
        """
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì†Œë¬¸ì ë³€í™˜
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text.lower())
        
        # ê³µë°±ìœ¼ë¡œ ë¶„í• 
        tokens = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§ (2ê¸€ì ì´ìƒ)
        filtered_tokens = [
            token for token in tokens 
            if len(token) >= 2 and token not in self.stop_words
        ]
        
        return filtered_tokens
    
    def add_file(self, file_path: str, content: str, file_info: Dict[str, Any]):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            content (str): íŒŒì¼ ë‚´ìš©
            file_info (Dict[str, Any]): íŒŒì¼ ì •ë³´
        """
        with self.lock:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ í•´ë‹¹ íŒŒì¼ ì œê±°
            self.remove_file(file_path)
            
            # íŒŒì¼ ì •ë³´ ì €ì¥
            self.file_info[file_path] = {
                **file_info,
                'indexed_time': datetime.now(),
                'content_preview': content[:200] if content else '',
                'full_content': content,  # ì „ì²´ ë‚´ìš©ë„ ì €ì¥ (ê²€ìƒ‰ìš©)
            }
            
            # íŒŒì¼ëª…ë„ ì¸ë±ì‹±ì— í¬í•¨
            filename = os.path.basename(file_path)
            all_content = f"{filename} {content}"
            
            # í…ìŠ¤íŠ¸ í† í°í™”
            tokens = self._tokenize(all_content)
            
            # ì—­ ì¸ë±ìŠ¤ êµ¬ì¶•
            for token in set(tokens):  # ì¤‘ë³µ ì œê±°
                self.index[token].add(file_path)
    
    def remove_file(self, file_path: str):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì œê±°í•  íŒŒì¼ ê²½ë¡œ
        """
        with self.lock:
            # íŒŒì¼ ì •ë³´ ì œê±°
            if file_path in self.file_info:
                del self.file_info[file_path]
            
            # ì¸ë±ìŠ¤ì—ì„œ í•´ë‹¹ íŒŒì¼ ì œê±°
            to_remove = []
            for token, file_paths in self.index.items():
                if file_path in file_paths:
                    file_paths.discard(file_path)
                    if not file_paths:  # ë¹ˆ ì§‘í•©ì´ë©´ í† í° ì œê±°
                        to_remove.append(token)
            
            for token in to_remove:
                del self.index[token]
    
    def search(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        with self.lock:
            if not query.strip():
                return []
            
            # ì¿¼ë¦¬ í† í°í™”
            query_tokens = self._tokenize(query)
            if not query_tokens:
                return []
            
            # ê° í† í°ë³„ë¡œ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ì°¾ê¸°
            token_results = []
            for token in query_tokens:
                matching_files = set()
                
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í°
                if token in self.index:
                    matching_files.update(self.index[token])
                
                # ë¶€ë¶„ ì¼ì¹˜í•˜ëŠ” í† í° (ì ‘ë‘ì‚¬ ë§¤ì¹­)
                for indexed_token in self.index:
                    if indexed_token.startswith(token) or token in indexed_token:
                        matching_files.update(self.index[indexed_token])
                
                token_results.append(matching_files)
            
            if not token_results:
                return []
            
            # AND ì—°ì‚° (ëª¨ë“  í† í°ì´ í¬í•¨ëœ íŒŒì¼)
            result_files = token_results[0]
            for token_result in token_results[1:]:
                result_files &= token_result
            
            # ê²°ê³¼ê°€ ì ìœ¼ë©´ OR ì—°ì‚°ë„ í¬í•¨
            if len(result_files) < max_results // 2:
                or_results = set()
                for token_result in token_results:
                    or_results |= token_result
                
                # AND ê²°ê³¼ë¥¼ ìš°ì„ í•˜ê³  OR ê²°ê³¼ë¥¼ ì¶”ê°€
                result_files = list(result_files) + list(or_results - result_files)
            else:
                result_files = list(result_files)
            
            # ê²°ê³¼ ì œí•œ
            result_files = result_files[:max_results]
            
            # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
            search_results = []
            for file_path in result_files:
                if file_path in self.file_info:
                    file_info = self.file_info[file_path]
                    
                    # ë§¤ì¹­ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content_preview = file_info.get('content_preview', '')
                    highlighted_preview = self._highlight_matches(content_preview, query_tokens)
                    
                    result = {
                        'file_path': file_path,
                        'filename': os.path.basename(file_path),
                        'file_type': file_info.get('file_type', 'unknown'),
                        'file_size_mb': file_info.get('file_size_mb', 0),
                        'indexed_time': file_info.get('indexed_time'),
                        'preview': highlighted_preview,
                        'relevance_score': self._calculate_relevance(file_path, query_tokens)
                    }
                    search_results.append(result)
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            search_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            return search_results
    
    def _highlight_matches(self, text: str, query_tokens: List[str]) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ëœ ë¶€ë¶„ì„ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            query_tokens (List[str]): ê²€ìƒ‰ í† í°ë“¤
            
        Returns:
            str: í•˜ì´ë¼ì´íŠ¸ëœ í…ìŠ¤íŠ¸
        """
        highlighted = text
        
        for token in query_tokens:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë§¤ì¹­
            pattern = re.compile(re.escape(token), re.IGNORECASE)
            highlighted = pattern.sub(f"**{token}**", highlighted)
        
        return highlighted
    
    def _calculate_relevance(self, file_path: str, query_tokens: List[str]) -> float:
        """
        íŒŒì¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            query_tokens (List[str]): ê²€ìƒ‰ í† í°ë“¤
            
        Returns:
            float: ê´€ë ¨ì„± ì ìˆ˜
        """
        if file_path not in self.file_info:
            return 0.0
        
        score = 0.0
        filename = os.path.basename(file_path).lower()
        
        for token in query_tokens:
            # íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
            if token in filename:
                score += 2.0
            
            # í† í° ë¹ˆë„ ì ìˆ˜
            if token in self.index:
                # ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ëŠ” í† í°ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                frequency = len(self.index[token])
                if frequency > 0:
                    score += 1.0 / frequency
        
        return score
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        with self.lock:
            return {
                'total_files': len(self.file_info),
                'total_tokens': len(self.index),
                'average_tokens_per_file': len(self.index) / max(len(self.file_info), 1),
                'file_types': self._get_file_type_distribution(),
            }
    
    def _get_file_type_distribution(self) -> Dict[str, int]:
        """íŒŒì¼ íƒ€ì…ë³„ ë¶„í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        distribution = defaultdict(int)
        for file_info in self.file_info.values():
            file_type = file_info.get('file_type', 'unknown')
            distribution[file_type] += 1
        return dict(distribution)


class SearchIndexer:
    """
    ê²€ìƒ‰ ì¸ë±ì„œ ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    íŒŒì¼ ì‹œìŠ¤í…œì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ìë™ìœ¼ë¡œ ì¸ë±ì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    JSON ìºì‹± ì‹œìŠ¤í…œìœ¼ë¡œ ë¹ ë¥¸ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """SearchIndexer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.file_manager = FileManager()
        self.index = SearchIndex()
        self.indexing_thread = None
        self.stop_indexing = False
        self.indexed_paths = set()
        
        # ğŸš€ JSON ìºì‹± ì‹œìŠ¤í…œ (ì‚¬ìš©ì ìš”ì²­)
        self.cache_directory = None
        self.cache_file_path = None
        self.metadata_file_path = None
    
    def index_directory(self, directory_path: str, recursive: bool = True, 
                       progress_callback=None):
        """
        ë””ë ‰í† ë¦¬ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤. (JSON ìºì‹± ì‹œìŠ¤í…œ í¬í•¨)
        
        Args:
            directory_path (str): ì¸ë±ì‹±í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            recursive (bool): í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨ ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°± í•¨ìˆ˜
        """
        if not os.path.exists(directory_path):
            return
        
        # ğŸš€ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ì‚¬ìš©ì ìš”ì²­: ë™ì¼ ê²½ë¡œì— JSON íŒŒì¼)
        self.set_cache_directory(directory_path)
        
        # ğŸ“‚ ìºì‹œì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        cache_loaded, files_to_reindex, new_files = self.load_index_from_cache(directory_path, recursive)
        
        print(f"ğŸ“‚ ë””ë ‰í† ë¦¬ ì¸ë±ì‹± ì‹œì‘: {directory_path}")
        if cache_loaded:
            print("âš¡ ìºì‹œì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œë¨. ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        start_time = time.time()
        indexed_count = 0
        
        try:
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            files_to_index = []
            
            if cache_loaded:
                # ğŸš€ ìºì‹œê°€ ìˆì„ ë•Œ: ë³€ê²½ëœ íŒŒì¼ + ìƒˆë¡œìš´ íŒŒì¼ë§Œ ì²˜ë¦¬
                files_to_index = files_to_reindex + new_files
                print(f"ğŸ¨ ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹±: ë³€ê²½ëœ íŒŒì¼ {len(files_to_reindex)}ê°œ + ìƒˆë¡œìš´ íŒŒì¼ {len(new_files)}ê°œ")
            else:
                # ğŸ’» ì²« ì¸ë±ì‹±: ì „ì²´ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
                if recursive:
                    for root, dirs, files in os.walk(directory_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            if self.file_manager.is_supported_file(file_path):
                                # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                                file_type = self.file_manager.get_file_type(file_path)
                                if file_type != 'excel':
                                    files_to_index.append(file_path)
                else:
                    for item in os.listdir(directory_path):
                        file_path = os.path.join(directory_path, item)
                        if os.path.isfile(file_path) and self.file_manager.is_supported_file(file_path):
                            # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                            file_type = self.file_manager.get_file_type(file_path)
                            if file_type != 'excel':
                                files_to_index.append(file_path)
            
            total_files = len(files_to_index)
            if cache_loaded:
                print(f"ğŸ“„ ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼: {total_files}ê°œ (ë³€ê²½/ì‹ ê·œ íŒŒì¼ë§Œ)")
            else:
                print(f"ğŸ“„ ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼: {total_files}ê°œ (ì „ì²´ íŒŒì¼)")
            
            # âš¡ ë¹ ë¥¸ ë°”ì´íŒ¨ìŠ¤: ì¸ë±ì‹±í•  íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë‹¨, ìºì‹œ ì—…ë°ì´íŠ¸ëŠ” í•„ìš”)
            if total_files == 0:
                print("ğŸ‰ ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì‹± ì™„ë£Œ!")
                # ì‚­ì œëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
                if cache_loaded:
                    self.save_index_to_cache()
                return
            
            # ğŸš€ ë©€í‹°ìŠ¤ë ˆë“œ ì¸ë±ì‹± (ì‚¬ìš©ì ìš”ì²­: 3-4ê°œ ìŠ¤ë ˆë“œë¡œ ì†ë„ ìµœëŒ€í™”)
            max_workers = min(4, max(1, len(files_to_index) // 10))  # ìµœì  ìŠ¤ë ˆë“œ ìˆ˜
            print(f"âš¡ {max_workers}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì¸ë±ì‹± ì‹œì‘...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # íŒŒì¼ë“¤ì„ ìŠ¤ë ˆë“œì— ë¶„ë°°
                futures = {
                    executor.submit(self._index_single_file, file_path): file_path 
                    for file_path in files_to_index
                }
                
                # ì™„ë£Œëœ ì‘ì—…ë“¤ ì²˜ë¦¬
                for i, future in enumerate(as_completed(futures)):
                    if self.stop_indexing:
                        break
                    
                    try:
                        result = future.result()
                        if result:  # ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ë¨
                            indexed_count += 1
                        
                        # ì§„í–‰ ìƒíƒœ ì½œë°±
                        if progress_callback:
                            progress = (i + 1) / total_files * 100
                            file_path = futures[future]
                            progress_callback(file_path, progress)
                    
                    except Exception as e:
                        file_path = futures[future]
                        print(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            
            elapsed_time = time.time() - start_time
            if cache_loaded:
                print(f"âœ… ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ íŒŒì¼ ì²˜ë¦¬, {elapsed_time:.2f}ì´ˆ ì†Œìš” (ìºì‹œ ì‚¬ìš©)")
            else:
                print(f"âœ… ì „ì²´ ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ íŒŒì¼, {elapsed_time:.2f}ì´ˆ ì†Œìš”")
            
            # ğŸš€ ì¸ë±ì‹± ì™„ë£Œ í›„ JSON ìºì‹œ ì €ì¥ (ì‚¬ìš©ì ìš”ì²­)
            if indexed_count > 0 or cache_loaded:
                self.save_index_to_cache()  # ìºì‹œê°€ ìˆì–´ë„ ì—…ë°ì´íŠ¸
            
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
    
    def search_files(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. (JSON ìºì‹œ ìš°ì„ , í´ë°±ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤)
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        # ğŸš€ JSON ìºì‹œì—ì„œ ìš°ì„  ê²€ìƒ‰ (ì‚¬ìš©ì ìš”ì²­: JSONì—ì„œ ë°”ë¡œ ê²€ìƒ‰)
        if self.cache_file_path and os.path.exists(self.cache_file_path):
            return self.search_files_from_json(query, max_results)
        
        # í´ë°±: ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        print("âš ï¸ JSON ìºì‹œ ì—†ìŒ. ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰...")
        return self.index.search(query, max_results)
    
    def add_file_to_index(self, file_path: str):
        """
        ê°œë³„ íŒŒì¼ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì¶”ê°€í•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            if self.file_manager.is_supported_file(file_path):
                # ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ (ì„±ëŠ¥ìƒ ì´ìœ )
                file_type = self.file_manager.get_file_type(file_path)
                if file_type == 'excel':
                    print(f"âš ï¸ ì—‘ì…€ íŒŒì¼ì€ ì¸ë±ì‹±ì—ì„œ ì œì™¸ë¨: {file_path}")
                    return
                
                file_info = self.file_manager.get_file_info(file_path)
                
                if file_info.get('supported', False):
                    content = self.file_manager.extract_text(file_path)
                    self.index.add_file(file_path, content, file_info)
                    self.indexed_paths.add(file_path)
                    print(f"âœ… íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ: {file_path}")
        
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
    
    def remove_file_from_index(self, file_path: str):
        """
        íŒŒì¼ì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì œê±°í•  íŒŒì¼ ê²½ë¡œ
        """
        self.index.remove_file(file_path)
        self.indexed_paths.discard(file_path)
        print(f"ğŸ—‘ï¸ íŒŒì¼ ì¸ë±ìŠ¤ ì œê±°: {file_path}")
    
    def update_file_in_index(self, file_path: str):
        """
        íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ì—…ë°ì´íŠ¸í•  íŒŒì¼ ê²½ë¡œ
        """
        if file_path in self.indexed_paths:
            self.remove_file_from_index(file_path)
        
        self.add_file_to_index(file_path)
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """
        ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        stats = self.index.get_statistics()
        stats['indexed_paths_count'] = len(self.indexed_paths)
        return stats
    
    def clear_index(self):
        """ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.index = SearchIndex()
        self.indexed_paths.clear()
        print("ğŸ§¹ ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def stop_indexing_process(self):
        """ì¸ë±ì‹± í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤."""
        self.stop_indexing = True
    
    def set_cache_directory(self, directory_path: str):
        """
        ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (ì‚¬ìš©ì ì œì•ˆ: ë™ì¼ ê²½ë¡œì— JSON íŒŒì¼ ì €ì¥)
        
        Args:
            directory_path (str): ê²€ìƒ‰ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.cache_directory = directory_path
        self.cache_file_path = os.path.join(directory_path, ".file_index.json")
        self.metadata_file_path = os.path.join(directory_path, ".index_metadata.json")
        print(f"ğŸ“ ìºì‹œ ì„¤ì •: {self.cache_file_path}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """
        íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤. (ìˆ˜ì • ì‹œê°„ + í¬ê¸° ê¸°ë°˜)
        
        Args:
            file_path (str): íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: íŒŒì¼ í•´ì‹œê°’
        """
        try:
            stat = os.stat(file_path)
            # ìˆ˜ì • ì‹œê°„ + í¬ê¸°ë¡œ í•´ì‹œ ìƒì„±
            hash_input = f"{stat.st_mtime}_{stat.st_size}_{file_path}"
            return hashlib.md5(hash_input.encode('utf-8')).hexdigest()
        except:
            return ""
    
    def save_index_to_cache(self):
        """
        ì¸ë±ìŠ¤ë¥¼ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤. (ì‚¬ìš©ì ìš”ì²­: JSON íŒŒì¼ë¡œ ìºì‹±)
        """
        if not self.cache_file_path or not self.cache_directory:
            print("âš ï¸ ìºì‹œ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return
        
        try:
            print("ğŸ’¾ ì¸ë±ìŠ¤ë¥¼ JSON íŒŒì¼ì— ì €ì¥ ì¤‘...")
            
            # ì¸ë±ìŠ¤ ë°ì´í„° êµ¬ì„± (ì‚¬ìš©ì ì œì•ˆ ë°©ì‹)
            cache_data = {
                "files": {},
                "last_indexed": datetime.now().isoformat(),
                "total_files": len(self.indexed_paths),
                "index_version": "1.0"
            }
            
            # íŒŒì¼ë³„ ì •ë³´ ì €ì¥ (íŒŒì¼ëª… + ë‚´ìš©)
            for file_path in self.indexed_paths:
                if file_path in self.index.file_info:
                    file_info = self.index.file_info[file_path]
                    relative_path = os.path.relpath(file_path, str(self.cache_directory))
                    
                    cache_data["files"][relative_path] = {
                        "content": file_info.get('full_content', ''),
                        "title": os.path.basename(file_path),
                        "size": file_info.get('file_size_mb', 0),
                        "modified": file_info.get('indexed_time', datetime.now()).isoformat(),
                        "type": file_info.get('file_type', 'unknown'),
                        "file_hash": self._get_file_hash(file_path),
                        "full_path": file_path
                    }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(str(self.cache_file_path), 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "cache_created": datetime.now().isoformat(),
                "indexed_directory": str(self.cache_directory),
                "total_files": len(self.indexed_paths),
                "cache_file_size": os.path.getsize(str(self.cache_file_path))
            }
            
            with open(str(self.metadata_file_path), 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.indexed_paths)}ê°œ íŒŒì¼")
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_index_from_cache(self, directory_path: str = None, recursive: bool = True) -> Tuple[bool, List[str], List[str]]:
        """
        JSON íŒŒì¼ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì‚¬ìš©ì ìš”ì²­: JSONì—ì„œ ë¹ ë¥¸ ê²€ìƒ‰)
        
        Args:
            directory_path (str): ë¹„êµí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒˆ íŒŒì¼ ê°ì§€ìš©)
            recursive (bool): í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨ ì—¬ë¶€
        
        Returns:
            tuple: (ë¡œë“œ ì„±ê³µ ì—¬ë¶€, ë³€ê²½ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸, ìƒˆë¡œìš´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸)
        """
        if not self.cache_file_path or not os.path.exists(self.cache_file_path):
            print("ğŸ“„ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì¸ë±ì‹±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return False, [], []
        
        try:
            print("ğŸ“‚ JSON ìºì‹œì—ì„œ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
            
            with open(str(self.cache_file_path), 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # ìºì‹œ ë²„ì „ ì²´í¬
            if cache_data.get("index_version") != "1.0":
                print("âš ï¸ ìºì‹œ ë²„ì „ ë¶ˆì¼ì¹˜. ìƒˆë¡œ ì¸ë±ì‹±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False, [], []
            
            # íŒŒì¼ ë³€ê²½ ì‚¬í•­ ì²´í¬ (ìŠ¤ë§ˆíŠ¸ ì¬ì¸ë±ì‹±)
            files_to_reindex = []
            valid_files = 0
            
            for relative_path, file_data in cache_data["files"].items():
                full_path = file_data.get("full_path")
                if not full_path or not os.path.exists(full_path):
                    continue
                
                # íŒŒì¼ í•´ì‹œ ì²´í¬ë¡œ ë³€ê²½ ê°ì§€
                current_hash = self._get_file_hash(full_path)
                cached_hash = file_data.get("file_hash", "")
                
                if current_hash != cached_hash:
                    files_to_reindex.append(full_path)
                else:
                    # ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ì€ ìºì‹œì—ì„œ ë³µì›
                    file_info = {
                        'file_type': file_data.get('type', 'unknown'),
                        'file_size_mb': file_data.get('size', 0),
                        'indexed_time': datetime.fromisoformat(file_data.get('modified', datetime.now().isoformat())),
                        'content_preview': file_data.get('content', ''),
                        'supported': True
                    }
                    
                    # ì¸ë±ìŠ¤ì— ì¶”ê°€
                    content = file_data.get('content', '')
                    self.index.add_file(full_path, content, file_info)
                    self.indexed_paths.add(full_path)
                    valid_files += 1
            
            print(f"âœ… ìºì‹œì—ì„œ {valid_files}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
            # ìƒˆë¡œìš´ íŒŒì¼ ë° ì‚­ì œëœ íŒŒì¼ ê°ì§€ (í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ ìºì‹œ ë¹„êµ)
            new_files = []
            deleted_files = []
            
            if directory_path:
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ì§€ì› íŒŒì¼ë“¤ ìˆ˜ì§‘ (recursive í”Œë˜ê·¸ ì¤€ìˆ˜)
                current_files = set()
                
                if recursive:
                    for root, dirs, files in os.walk(directory_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            if self.file_manager.is_supported_file(file_path):
                                file_type = self.file_manager.get_file_type(file_path)
                                if file_type != 'excel':  # ì—‘ì…€ íŒŒì¼ ì œì™¸
                                    # ğŸ”§ ê²½ë¡œ ì •ê·œí™”: ì¼ê´€ì„± ìˆëŠ” ë¹„êµë¥¼ ìœ„í•´
                                    normalized_path = os.path.normcase(os.path.normpath(os.path.realpath(file_path)))
                                    current_files.add(normalized_path)
                else:
                    for item in os.listdir(directory_path):
                        file_path = os.path.join(directory_path, item)
                        if os.path.isfile(file_path) and self.file_manager.is_supported_file(file_path):
                            file_type = self.file_manager.get_file_type(file_path)
                            if file_type != 'excel':  # ì—‘ì…€ íŒŒì¼ ì œì™¸
                                # ğŸ”§ ê²½ë¡œ ì •ê·œí™”: ì¼ê´€ì„± ìˆëŠ” ë¹„êµë¥¼ ìœ„í•´
                                normalized_path = os.path.normcase(os.path.normpath(os.path.realpath(file_path)))
                                current_files.add(normalized_path)
                
                # ğŸš¨ ì¤‘ìš”: ì‚­ì œ ê°ì§€ ë²”ìœ„ë¥¼ current_filesì™€ ë™ì¼í•˜ê²Œ ì œí•œ
                # recursive=Falseì¼ ë•Œ í•˜ìœ„í´ë” íŒŒì¼ì„ "ì‚­ì œë¨"ìœ¼ë¡œ ì˜ëª» íŒë‹¨í•˜ëŠ” ë²„ê·¸ ë°©ì§€
                
                # ê²½ë¡œ ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, êµ¬ë¶„ì, ì‹¬ë³¼ë¦­ë§í¬ ì²˜ë¦¬)
                normalized_directory = os.path.normcase(os.path.normpath(os.path.realpath(directory_path)))
                
                if recursive:
                    # recursive=True: ëª¨ë“  ìºì‹œëœ íŒŒì¼ ê³ ë ¤ (ì •ê·œí™”)
                    cached_files = set()
                    for file_data in cache_data["files"].values():
                        cached_path = file_data.get("full_path")
                        if cached_path:
                            # ğŸ”§ ìºì‹œëœ ê²½ë¡œë„ ì •ê·œí™”: current_filesì™€ ì¼ê´€ì„± ìœ ì§€
                            normalized_cached = os.path.normcase(os.path.normpath(os.path.realpath(cached_path)))
                            cached_files.add(normalized_cached)
                else:
                    # recursive=False: í˜„ì¬ í´ë”ì˜ ìºì‹œëœ íŒŒì¼ë§Œ ê³ ë ¤ (ì •ê·œí™”ëœ ê²½ë¡œ ë¹„êµ)
                    cached_files = set()
                    for file_data in cache_data["files"].values():
                        cached_path = file_data.get("full_path")
                        if cached_path:
                            # ğŸ”§ ìºì‹œëœ ê²½ë¡œ ì •ê·œí™”
                            normalized_cached = os.path.normcase(os.path.normpath(os.path.realpath(cached_path)))
                            cached_parent = os.path.normcase(os.path.normpath(os.path.realpath(os.path.dirname(cached_path))))
                            if cached_parent == normalized_directory:
                                cached_files.add(normalized_cached)
                
                # ìƒˆë¡œìš´ íŒŒì¼ = í˜„ì¬ íŒŒì¼ - ìºì‹œëœ íŒŒì¼ (ì •ê·œí™”ëœ ê²½ë¡œë¡œ ì •í™•í•œ ë¹„êµ)
                new_files_normalized = list(current_files - cached_files)
                
                # ì‚­ì œëœ íŒŒì¼ = ìºì‹œëœ íŒŒì¼ - í˜„ì¬ íŒŒì¼ (ì •ê·œí™”ëœ ê²½ë¡œë¡œ ì •í™•í•œ ë¹„êµ)
                deleted_files_normalized = list(cached_files - current_files)
                
                # ğŸ”„ ì¸ë±ì‹±ì„ ìœ„í•´ ì›ë³¸ ì ˆëŒ€ ê²½ë¡œë¡œ ë³µì› (ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ë³¸ ê²½ë¡œ ì‚¬ìš©)
                # new_files: ì •ê·œí™”ëœ ê²½ë¡œì—ì„œ ì›ë³¸ ì ˆëŒ€ ê²½ë¡œ ë§¤í•‘
                normalized_to_original = {}
                if recursive:
                    for root, dirs, files in os.walk(directory_path):
                        for file in files:
                            original_path = os.path.join(root, file)
                            if self.file_manager.is_supported_file(original_path):
                                file_type = self.file_manager.get_file_type(original_path)
                                if file_type != 'excel':
                                    normalized = os.path.normcase(os.path.normpath(os.path.realpath(original_path)))
                                    normalized_to_original[normalized] = original_path
                else:
                    for item in os.listdir(directory_path):
                        original_path = os.path.join(directory_path, item)
                        if os.path.isfile(original_path) and self.file_manager.is_supported_file(original_path):
                            file_type = self.file_manager.get_file_type(original_path)
                            if file_type != 'excel':
                                normalized = os.path.normcase(os.path.normpath(os.path.realpath(original_path)))
                                normalized_to_original[normalized] = original_path
                
                # ì›ë³¸ ê²½ë¡œë¡œ ë³µì›
                new_files = [normalized_to_original.get(norm_path, norm_path) for norm_path in new_files_normalized]
                deleted_files = deleted_files_normalized  # ì‚­ì œëœ íŒŒì¼ì€ ì •ê·œí™”ëœ ê²½ë¡œ ì‚¬ìš©
                
                # ì‚­ì œëœ íŒŒì¼ì„ ì¸ë±ìŠ¤ì—ì„œ ì œê±°
                for deleted_file in deleted_files:
                    if deleted_file in self.indexed_paths:
                        self.remove_file_from_index(deleted_file)
            
            if files_to_reindex:
                print(f"ğŸ”„ ë³€ê²½ëœ íŒŒì¼ {len(files_to_reindex)}ê°œ ì¬ì¸ë±ì‹± í•„ìš”")
            
            if new_files:
                print(f"ğŸ“„ ìƒˆë¡œìš´ íŒŒì¼ {len(new_files)}ê°œ ë°œê²¬")
            
            if deleted_files:
                print(f"ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼ {len(deleted_files)}ê°œ ì œê±°")
            
            return True, files_to_reindex, new_files
            
        except Exception as e:
            print(f"âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False, [], []
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """
        ìºì‹œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: ìºì‹œ í†µê³„
        """
        from typing import Dict, Any, Union
        stats: Dict[str, Union[bool, float, str]] = {"cache_available": False}
        
        if self.cache_file_path and os.path.exists(str(self.cache_file_path)):
            try:
                cache_size = os.path.getsize(str(self.cache_file_path))
                cache_modified = datetime.fromtimestamp(os.path.getmtime(str(self.cache_file_path)))
                
                stats["cache_available"] = True
                stats["cache_size_mb"] = float(cache_size) / (1024.0 * 1024.0)  # ëª…ì‹œì  float ë³€í™˜
                stats["cache_modified"] = cache_modified.isoformat()
                stats["cache_file"] = str(self.cache_file_path)
                
            except:
                pass
        
        return stats
    
    def _index_single_file(self, file_path: str) -> bool:
        """
        ë‹¨ì¼ íŒŒì¼ì„ ì¸ë±ì‹±í•©ë‹ˆë‹¤. (ë©€í‹°ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©)
        
        Args:
            file_path (str): ì¸ë±ì‹±í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            bool: ì¸ë±ì‹± ì„±ê³µ ì—¬ë¶€
        """
        try:
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            file_info = self.file_manager.get_file_info(file_path)
            
            if file_info.get('supported', False):
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = self.file_manager.extract_text(file_path)
                
                # ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ì— ì¶”ê°€
                self.index.add_file(file_path, content, file_info)
                self.indexed_paths.add(file_path)
                
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì¸ë±ì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            return False
    
    def search_files_from_json(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        JSON ìºì‹œì—ì„œ ì§ì ‘ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ì‚¬ìš©ì ìš”ì²­: JSONì—ì„œ ë°”ë¡œ ë¹ ë¥¸ ê²€ìƒ‰)
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.cache_file_path or not os.path.exists(self.cache_file_path):
            print("ğŸ“„ JSON ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì‹±ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []
        
        try:
            print(f"ğŸ” JSONì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            
            with open(str(self.cache_file_path), 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            results = []
            query_lower = query.lower()
            
            # íŒŒì¼ë³„ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            for relative_path, file_data in cache_data.get("files", {}).items():
                full_path = file_data.get("full_path", "")
                if not os.path.exists(full_path):
                    continue
                
                # íŒŒì¼ëª… + ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                title = file_data.get("title", "").lower()
                content = file_data.get("content", "").lower()
                
                # ë§¤ì¹­ ì²´í¬
                filename_match = query_lower in title
                content_match = query_lower in content
                
                if filename_match or content_match:
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                    relevance_score = 0.0
                    if filename_match:
                        relevance_score += 2.0  # íŒŒì¼ëª… ë§¤ì¹­ì€ ë†’ì€ ì ìˆ˜
                    if content_match:
                        relevance_score += 1.0  # ë‚´ìš© ë§¤ì¹­
                    
                    # ë§¤ì¹­ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    preview = self._extract_context_from_content(
                        file_data.get("content", ""), query
                    )
                    
                    result = {
                        'file_path': full_path,
                        'filename': file_data.get("title", ""),
                        'file_type': file_data.get("type", "unknown"),
                        'file_size_mb': file_data.get("size", 0),
                        'indexed_time': file_data.get("modified", ""),
                        'preview': preview,
                        'relevance_score': relevance_score
                    }
                    results.append(result)
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            results.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            print(f"âœ… JSON ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results[:max_results]
            
        except Exception as e:
            print(f"âŒ JSON ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_files_by_filename_from_json(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        JSON ìºì‹œì—ì„œ íŒŒì¼ëª…ìœ¼ë¡œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ì´ˆê³ ì†)
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.cache_file_path or not os.path.exists(self.cache_file_path):
            return []
        
        try:
            with open(str(self.cache_file_path), 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            results = []
            query_lower = query.lower()
            
            # íŒŒì¼ëª…ì—ì„œë§Œ ê²€ìƒ‰ (ë§¤ìš° ë¹ ë¦„)
            for relative_path, file_data in cache_data.get("files", {}).items():
                full_path = file_data.get("full_path", "")
                if not os.path.exists(full_path):
                    continue
                
                title = file_data.get("title", "").lower()
                filename_without_ext = os.path.splitext(title)[0].lower()
                
                if query_lower in filename_without_ext:
                    relevance_score = 1.0
                    if filename_without_ext.startswith(query_lower):
                        relevance_score = 2.0  # ì‹œì‘í•˜ëŠ” ê²½ìš° ë” ë†’ì€ ì ìˆ˜
                    
                    result = {
                        'file_path': full_path,
                        'filename': file_data.get("title", ""),
                        'file_type': file_data.get("type", "unknown"),
                        'file_size_mb': file_data.get("size", 0),
                        'indexed_time': file_data.get("modified", ""),
                        'preview': f"íŒŒì¼ëª… ë§¤ì¹­: {file_data.get('title', '')}",
                        'relevance_score': relevance_score
                    }
                    results.append(result)
            
            results.sort(key=lambda x: x['relevance_score'], reverse=True)
            return results[:max_results]
            
        except Exception as e:
            print(f"âŒ JSON íŒŒì¼ëª… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_context_from_content(self, content: str, query: str, context_length: int = 150) -> str:
        """
        ê²€ìƒ‰ì–´ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            content (str): ì›ë³¸ ë‚´ìš©
            query (str): ê²€ìƒ‰ì–´
            context_length (int): ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            str: í•˜ì´ë¼ì´íŠ¸ëœ ì»¨í…ìŠ¤íŠ¸
        """
        if not content or not query:
            return content[:context_length] if content else ""
        
        content_lower = content.lower()
        query_lower = query.lower()
        
        # ì²« ë²ˆì§¸ ë§¤ì¹­ ìœ„ì¹˜ ì°¾ê¸°
        match_pos = content_lower.find(query_lower)
        if match_pos == -1:
            return content[:context_length]
        
        # ì»¨í…ìŠ¤íŠ¸ ì‹œì‘/ë ìœ„ì¹˜ ê³„ì‚°
        start = max(0, match_pos - context_length // 2)
        end = min(len(content), match_pos + len(query) + context_length // 2)
        
        context = content[start:end]
        
        # ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŠ¸ (ê°„ë‹¨í•œ ë°©ì‹)
        highlighted = context.replace(
            content[match_pos:match_pos + len(query)], 
            f"**{content[match_pos:match_pos + len(query)]}**"
        )
        
        # ì•ë’¤ ìƒëµ í‘œì‹œ
        if start > 0:
            highlighted = "..." + highlighted
        if end < len(content):
            highlighted = highlighted + "..."
        
        return highlighted